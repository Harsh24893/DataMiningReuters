'''This script read the files created by the offline_processor.csv script. Specifically, it reads, training_data_file.csv, testing_data_file.csv and neighbours.csv
files. It uses the information in the neighbours.csv file to compute the posterior probabilities. This script only reads the files which are generated by the
offline_processor.csv and writes the prediction results into ml_knn_predictions.csv file, with in the same folder.'''
from sklearn.datasets import load_iris
from sklearn import cross_validation
import numpy as np
import csv
import collections
import math
import operator
from operator import itemgetter
from sklearn.metrics import classification_report, accuracy_score
from collections import Counter
import random
import time
import sys

# function to compute the euclidean distance between two data vector instances

def get_euclidean_distance(training_instance,testing_instance) :
    train_vector = training_instance[0]
    test_vector = testing_instance[0]
    points = zip(train_vector,test_vector)
    euclidean_dist = math.sqrt(sum([pow(int(a) - int(b), 2) for (a, b) in points]))
    return euclidean_dist

# function to return the k nearest neighbors of the given data vector which is passed as a parameter. The sorting of the vectors is done based on euclidean distance

def nearest_neighbours(test_instance,training_set,k) :

    tuple_distance_list = []
    for train_instance in training_set :
        dist = get_euclidean_distance(train_instance,test_instance)
        tuple_distance_list.append([train_instance, dist])
    sorted_tuple_distance_list = sorted(tuple_distance_list,key=operator.itemgetter(1))
    k_nearest_neighbours = []
    for i in range(0,k):
        k_nearest_neighbours.append(sorted_tuple_distance_list[i][0])

    return k_nearest_neighbours

# function to compute the posterior probabilities according to the definition, from the data structures so far initialized.
# function returns the posterior probabilitites as a three dimensional array, with the first dimension determinig the number of labels, the second
# being equal to k+1, and the third dimension account for the presence or absence of that label for that vector.

def compute_posterior_probabilities(training_set,nbrs,k,N,C):

    posterior = [[[0 for z in range(2)] for y in range(k+1)] for x in range(N)]

    m = len(training_set)
    for l in range(0,N):
        for j in range(0,k+1):
            # c[j], for each l, counts the number of training instance with label l, whose k-nearest neighbours contain exactly
            # j instances of label l. Similarly, c1[j] counts how many training instances without label l, have exactly j instances
            # of label l, among their k nearest neighbours.
            c = [0]*(k+1)
            c1 = [0]*(k+1)
            for i in range(0,m) :
                # for every i, delta counts how many of the neighbours of the ith training instance, belong to the label class l.
                # Here, C[] is the  set of all the member ship counting vectors of all the training instances.
                delta = C[i][l]
                if(training_set[i][1][l]==1):
                    c[delta] += 1
                else : c1[delta] += 1
        c_sum = sum(c)
        c1_sum = sum(c1)

        # posterior probabilities arenow computed according to the definition, presented in the paper.

        for j in range(0,k+1) :
            posterior[l][j][1] = float(1+c[j])/float((k+1)+c_sum)
            posterior[l][j][0] = float(1+c1[j])/float((k+1)+c1_sum)

    return posterior

# function to return the prediction vector of the inputted testing instance vector. This funciton also computes the
# prior probabilities, in the process.

def knn_classify(test_instance,training_set,k,global_nbrs,C):

    neighbours = nearest_neighbours(test_instance,training_set,k)
    labels = []
    for n in neighbours :
        labels.append(tuple(n[1]))
    # compute the prediction based on maximum aposteriori principle
    N = len(labels[0])
    final_prediction = [0]*N
    aposteriori = []
    prior = [0]*N;
    posterior = [[[0 for z in range(2)] for y in range(k + 1)] for x in range(N)]
    # first compute the prior probabilities
    # prior[] will be of size N, which contains the probability of a 1 being at that position
    M = len(training_set) # total number of training instances
    for i in range(0,N):
        c = 0 # to keep a count of how many training instances have a 1 at that position
        for t in training_set:
            if(t[1][i]==1) :
                c = c+1
        prior[i] = float(c+1)/float(M+2);

    # We shall now compute the posterior probabalities
    posterior = compute_posterior_probabilities(training_set,global_nbrs,k,N,C)
    # compute the membership counting vector for the particular testing instance. Remember that C[] is the set of all such membership_counting vectors.
    prediction = [0]*N
    membership_counting_vector = [0]*N
    for l in range(0,N):
        for n in neighbours :
            if(n[1][l]==1):
                membership_counting_vector[l]+=1
        if(math.log(prior[l]*posterior[l][membership_counting_vector[l]][1]) >= math.log((1-prior[l])*posterior[l][membership_counting_vector[l]][0])):
            prediction[l] = 1
        else : prediction[l] = 0
    '''final_prediction = [0]*N
    int_prediction = [0] * N
    for i in range(0,N) :
        final_prediction[i] = float(prior[i])*posterior[i]
        if(final_prediction[i]>(1/float(N))) :
            int_prediction[i] = 1
        else :
            int_prediction[i] = 0
    prediction = tuple(int_prediction)
    #prediction = Counter(labels).most_common()[0][0]'''
    return prediction



def main() :

    # This script uses k=5 as the default value, to avoid expensive computation.
    k = 5
    final_data = {}
    final_data['data'] = []
    final_data['targets'] = []
    unique_label_map = {}  # this will later be converted to a list for iteration purposes

    train = []
    test = []
    test_labels = []
    train_file_count = 0
    test_file_count = 0
    # read the training and the testing data from the files
    print "Reading the Training and Testing Data"
    if(len(sys.argv)==1):
        folder_path = '/home/8/athmakuri.1/athmakuri.1_DM_Lab_2/sample1'
    else :
        folder_index = sys.argv[1]
        if(folder_index=='-n'):
            folder_path = '/home/8/athmakuri.1/athmakuri.1_DM_Lab_2/new_offline_data'
        else : folder_path = '/home/8/athmakuri.1/athmakuri.1_DM_Lab_2/sample'+folder_index
    '''========================================================================================================================================================'''

    with open(folder_path+'/training_data_file.csv') as train_fp :
        train_reader = csv.reader(train_fp,delimiter=',')
        for line in train_reader :
            bar_index = line.index('|')
            l1 = line[1:bar_index]
            l2 = line[bar_index+1:]
            l1 = [int(i) for i in l1]
            l2 = [int(i) for i in l2]
            train.append((l1,l2))
            train_file_count += 1

    with open(folder_path+'/testing_data_file.csv') as test_fp :
        test_reader = csv.reader(test_fp,delimiter=',')
        for line in test_reader :
            bar_index = line.index('|')
            l1 = line[1:bar_index]
            l2 = line[bar_index+1:]
            l1 = [int(i) for i in l1]
            l2 = [int(i) for i in l2]
            test_labels.append(l2)
            test.append((l1,l2))
            test_file_count += 1
    print "number of training instances : ",train_file_count
    print "number of test instances : ",test_file_count
    print "time taken to read training and testing files : ",time.time()-start
    '''========================================================================================================================================================'''
    # initialize nbrs, by reading the neighbours file. nbrs[] stores all the information in the neighbours.csv file in the main memory.
    nbrs = []

    with open(folder_path+'/neighbours.csv', 'r') as nb_fp:
        nb_reader = csv.reader(nb_fp, delimiter=',')
        lc = 1
        temp = []
        for line in nb_reader:
            bar_index = line.index('|')
            l1 = line[1:bar_index]
            l2 = line[bar_index + 1:]
            fl1 = [int(i) for i in l1]
            fl2 = [int(i) for i in l2]
            temp.append((fl1, fl2))
            if (lc % k == 0):
                nbrs.append(temp)
                temp = []
            lc += 1

    print "neighbours of training set initialized"
    print "time taken for reading the neighbours of the training instances : ",time.time()-start
    C = []
    N = len(train[0][1]) # Number of labels
    M = len(train) # number of training examples
    # compute the membership counting vectors for all the training examples
    for i in range(0,M) :
        membership_count = [0] * N
        for l in range(0,N):
            for n in nbrs[i]:
                if(n[1][l]==1):
                    membership_count[l] += 1
        C.append(membership_count)

    predictions = []

    print "                     **********online processing begins**********                        "

    print "classifying test instances"

    for testing_instance in test :
        predictions.append(knn_classify(testing_instance,train,k,nbrs,C))

    print  "classification of test instances completed"
    print "time taken to classify test instances : ",time.time()-start
    print "                     **********performance measures**********                        "
    bool_predictions = []
    for i in range(0,len(predictions)) :
        if(cmp(list(predictions[i]),test_labels[i]) == 0) :
            bool_predictions.append(1)
        else : bool_predictions.append(0)
    total_correct_predictions = sum(bool_predictions)
    accuracy = (float(total_correct_predictions)/len(predictions))*100

    print "writing the predictions to the ml_knn_predictions.csv file"
    with open('/home/8/athmakuri.1/athmakuri.1_DM_Lab_2/ml_knn_predictions.csv','wb') as mlknn_fp :
        mlknn_writer = csv.writer(mlknn_fp,delimiter=',')
        mlknn_writer.writerows(predictions)
    print "writint to file completed"

    print "number of test instances : ",len(predictions)
    print "number of instances predicted correctly : ",total_correct_predictions
    print "number of instances incorrectly predicted : ",len(predictions)-total_correct_predictions
    print "the accuracy is : ",accuracy,"%"



if __name__ == "__main__":
    start = time.time()
    print "********************************************************Begin Processing***********************************************************************"
    main()
    end = time.time()
    print "********************************************************End Processing*************************************************************************"
    print "total time for the complete processing : ",end-start;

